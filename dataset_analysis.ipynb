{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "import wandb\n",
    "\n",
    "from decimal import Decimal\n",
    "from glob import glob\n",
    "from datasets import Dataset, Audio, load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"mozilla-foundation/common_voice_11_0\"\n",
    "NAME = \"cs\"\n",
    "SPLIT = \"test\"\n",
    "SAMPLING_RATE = 16_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = load_dataset(DATASET_NAME, NAME, split=SPLIT)\n",
    "hf_dataset = hf_dataset.cast_column(\"audio\", Audio(sampling_rate=SAMPLING_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_lenghts = np.array([len(example['audio']['array']) for example in tqdm(hf_dataset.to_iterable_dataset(), total=len(hf_dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_lenghts_secs = wav_lenghts/SAMPLING_RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(wav_lenghts_secs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_secs = np.sum(wav_lenghts_secs)\n",
    "total_secs, total_secs/3600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training table sumarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"wav2vec2\" # wav2vec2 / t5\n",
    "train_paths = glob(f'/home/sulcm/models/{model}/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_latex_table(results: dict, best_results: dict) -> str:\n",
    "    formated_table = ''\n",
    "    for run, metrics in results.items():\n",
    "        formated_table += run + ' & ' + ' & '.join([f'\\\\textbf{{{v}}}' if (m in best_results and run == best_results[m][0]) else str(v) for m, v in metrics.items()]) + ' \\\\\\\\\\n'\n",
    "    \n",
    "    return formated_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['wer', 'cer']\n",
    "best_results = dict.fromkeys(metrics, ('', 1.0))\n",
    "table_prep = {}\n",
    "\n",
    "wandb_api = wandb.Api()\n",
    "runs = wandb_api.runs(\"sulcm/huggingface\")\n",
    "run_params = [\"learning_rate\", \"batch_size\", \"max_steps\"]\n",
    "\n",
    "for train_res in train_paths:\n",
    "    path2res = train_res + '/all_results.json'\n",
    "    if not os.path.exists(path2res):\n",
    "        continue\n",
    "\n",
    "    with open(path2res, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    table_prep[train_res.split('/')[-1].split('-')[-1]] = {m: f'{100.0*results[f\"eval_{m}\"]:.02f}' for m in metrics}\n",
    "    for run in runs:\n",
    "        if run.state != \"finished\":\n",
    "            continue\n",
    "        if run.config[\"run_name\"].split(\"/\")[-2] != train_res.split('/')[-1]:\n",
    "            continue\n",
    "        run_params_dict = {}\n",
    "        for param in run_params:\n",
    "            if param == \"batch_size\":\n",
    "                run_params_dict[param] = run.config[\"per_device_train_batch_size\"] * run.config[\"gradient_accumulation_steps\"]\n",
    "            elif param == \"learning_rate\":\n",
    "                run_params_dict[param] = \"%.2e\" % Decimal(run.config[param])\n",
    "            else:\n",
    "                run_params_dict[param] = run.config[param]\n",
    "        table_prep[train_res.split('/')[-1].split('-')[-1]].update(run_params_dict)\n",
    "        break\n",
    "        \n",
    "\n",
    "    for metric in metrics:\n",
    "        if best_results[metric][1] > results[f\"eval_{metric}\"]:\n",
    "            best_results[metric] = (train_res.split('/')[-1].split('-')[-1], results[f\"eval_{metric}\"])\n",
    "\n",
    "print(table_prep)\n",
    "print(best_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(format_latex_table(table_prep, best_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
