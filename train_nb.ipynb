{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers>=4.34.0 in /auto/plzen4-ntis/home/sulcm01/.local-tensorflow23.08-r1.simg/lib/python3.10/site-packages (from -r ./requirements.txt (line 1)) (4.34.1)\n",
      "Requirement already satisfied: datasets>=1.18.0 in /auto/plzen4-ntis/home/sulcm01/.local-tensorflow23.08-r1.simg/lib/python3.10/site-packages (from -r ./requirements.txt (line 2)) (2.14.6)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r ./requirements.txt (line 3)) (2.0.1)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from -r ./requirements.txt (line 4)) (2.0.2)\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from -r ./requirements.txt (line 5)) (0.10.0)\n",
      "Requirement already satisfied: jiwer in /auto/plzen4-ntis/home/sulcm01/.local-tensorflow23.08-r1.simg/lib/python3.10/site-packages (from -r ./requirements.txt (line 6)) (3.0.3)\n",
      "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (from -r ./requirements.txt (line 7)) (2.3.1)\n",
      "Requirement already satisfied: evaluate in /auto/plzen4-ntis/home/sulcm01/.local-tensorflow23.08-r1.simg/lib/python3.10/site-packages (from -r ./requirements.txt (line 8)) (0.4.1)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from -r ./requirements.txt (line 9)) (0.23.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from -r ./requirements.txt (line 10)) (0.1.99)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from -r ./requirements.txt (line 11)) (4.21.12)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r ./requirements.txt (line 1)) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r ./requirements.txt (line 1)) (0.17.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r ./requirements.txt (line 1)) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r ./requirements.txt (line 1)) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r ./requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r ./requirements.txt (line 1)) (2023.8.8)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r ./requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /auto/plzen4-ntis/home/sulcm01/.local-tensorflow23.08-r1.simg/lib/python3.10/site-packages (from transformers>=4.34.0->-r ./requirements.txt (line 1)) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r ./requirements.txt (line 1)) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r ./requirements.txt (line 1)) (4.66.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.18.0->-r ./requirements.txt (line 2)) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /auto/plzen4-ntis/home/sulcm01/.local-tensorflow23.08-r1.simg/lib/python3.10/site-packages (from datasets>=1.18.0->-r ./requirements.txt (line 2)) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=1.18.0->-r ./requirements.txt (line 2)) (1.5.2)\n",
      "Requirement already satisfied: xxhash in /auto/plzen4-ntis/home/sulcm01/.local-tensorflow23.08-r1.simg/lib/python3.10/site-packages (from datasets>=1.18.0->-r ./requirements.txt (line 2)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /auto/plzen4-ntis/home/sulcm01/.local-tensorflow23.08-r1.simg/lib/python3.10/site-packages (from datasets>=1.18.0->-r ./requirements.txt (line 2)) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.18.0->-r ./requirements.txt (line 2)) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=1.18.0->-r ./requirements.txt (line 2)) (3.8.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r ./requirements.txt (line 3)) (4.7.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r ./requirements.txt (line 3)) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r ./requirements.txt (line 3)) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r ./requirements.txt (line 3)) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch->-r ./requirements.txt (line 3)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch->-r ./requirements.txt (line 3)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch->-r ./requirements.txt (line 3)) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch->-r ./requirements.txt (line 3)) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch->-r ./requirements.txt (line 3)) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch->-r ./requirements.txt (line 3)) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch->-r ./requirements.txt (line 3)) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch->-r ./requirements.txt (line 3)) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch->-r ./requirements.txt (line 3)) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch->-r ./requirements.txt (line 3)) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch->-r ./requirements.txt (line 3)) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r ./requirements.txt (line 3)) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->-r ./requirements.txt (line 3)) (68.0.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->-r ./requirements.txt (line 3)) (0.41.1)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r ./requirements.txt (line 3)) (3.27.5)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r ./requirements.txt (line 3)) (16.0.6)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->-r ./requirements.txt (line 5)) (3.0.0)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r ./requirements.txt (line 5)) (1.11.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r ./requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa->-r ./requirements.txt (line 5)) (1.3.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r ./requirements.txt (line 5)) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r ./requirements.txt (line 5)) (0.57.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa->-r ./requirements.txt (line 5)) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r ./requirements.txt (line 5)) (1.7.0)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->-r ./requirements.txt (line 5)) (0.3.6)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->-r ./requirements.txt (line 5)) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r ./requirements.txt (line 5)) (1.0.5)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer->-r ./requirements.txt (line 6)) (8.1.5)\n",
      "Requirement already satisfied: rapidfuzz<4,>=3 in /auto/plzen4-ntis/home/sulcm01/.local-tensorflow23.08-r1.simg/lib/python3.10/site-packages (from jiwer->-r ./requirements.txt (line 6)) (3.4.0)\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu->-r ./requirements.txt (line 7)) (2.8.2)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->-r ./requirements.txt (line 7)) (0.9.0)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu->-r ./requirements.txt (line 7)) (0.4.6)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->-r ./requirements.txt (line 7)) (4.9.3)\n",
      "Requirement already satisfied: responses<0.19 in /auto/plzen4-ntis/home/sulcm01/.local-tensorflow23.08-r1.simg/lib/python3.10/site-packages (from evaluate->-r ./requirements.txt (line 8)) (0.18.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r ./requirements.txt (line 9)) (5.9.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.18.0->-r ./requirements.txt (line 2)) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.18.0->-r ./requirements.txt (line 2)) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.18.0->-r ./requirements.txt (line 2)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.18.0->-r ./requirements.txt (line 2)) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.18.0->-r ./requirements.txt (line 2)) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.18.0->-r ./requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.18.0->-r ./requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->-r ./requirements.txt (line 5)) (0.40.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa->-r ./requirements.txt (line 5)) (3.10.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34.0->-r ./requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34.0->-r ./requirements.txt (line 1)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34.0->-r ./requirements.txt (line 1)) (2023.7.22)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->-r ./requirements.txt (line 5)) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa->-r ./requirements.txt (line 5)) (1.15.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r ./requirements.txt (line 3)) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.18.0->-r ./requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.18.0->-r ./requirements.txt (line 2)) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r ./requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->-r ./requirements.txt (line 5)) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.18.0->-r ./requirements.txt (line 2)) (1.16.0)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: torchsde 0.2.5 has a non-standard dependency specifier numpy>=1.19.*; python_version >= \"3.7\". pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of torchsde or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# install required pckgs\n",
    "! pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/auto/plzen4-ntis/home/sulcm01/Repos/wav2vec2-w-T5-cs\n"
     ]
    }
   ],
   "source": [
    "# check where am I\n",
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4804\n",
      "-rw------- 1 sulcm01 meta 1050506 Dec  4 15:27 JupyterLab_23.08.e18928489\n",
      "-rw------- 1 sulcm01 meta  213715 Dec  6 13:31 JupyterLab_23.08.e19031396\n",
      "-rw------- 1 sulcm01 meta  215236 Dec  7 18:13 JupyterLab_23.08.e19046614\n",
      "-rw------- 1 sulcm01 meta  586522 Dec  9 16:22 JupyterLab_23.08.e19060535\n",
      "-rw------- 1 sulcm01 meta  220037 Dec 10 20:25 JupyterLab_23.08.e19063589\n",
      "-rw------- 1 sulcm01 meta  218856 Dec 15 09:05 JupyterLab_23.08.e19114179\n",
      "-rw------- 1 sulcm01 meta  215785 Dec 19 09:09 JupyterLab_23.08.e19154839\n",
      "-rw------- 1 sulcm01 meta  217387 Dec 20 10:48 JupyterLab_23.08.e19161205\n",
      "-rw------- 1 sulcm01 meta  222809 Jan  5 10:28 JupyterLab_23.08.e19430156\n",
      "-rw------- 1 sulcm01 meta  217574 Jan  7 12:33 JupyterLab_23.08.e19476417\n",
      "-rw------- 1 sulcm01 meta  229231 Jan 12 14:35 JupyterLab_23.08.e19620529\n",
      "-rw------- 1 sulcm01 meta  213494 Jan 24 16:18 JupyterLab_23.08.e19821510\n",
      "-rw------- 1 sulcm01 meta  212798 Jan 26 11:34 JupyterLab_23.08.e19836725\n",
      "-rw------- 1 sulcm01 meta  288757 Jan 28 16:20 JupyterLab_23.08.e19867443\n",
      "-rw------- 1 sulcm01 meta  132408 Feb 19 00:07 JupyterLab_23.08.e20223978\n",
      "-rw------- 1 sulcm01 meta  102487 Feb 20 19:42 JupyterLab_23.08.e20250938\n",
      "-rw------- 1 sulcm01 meta  144635 Feb 22 00:12 JupyterLab_23.08.e20259738\n",
      "-rw------- 1 sulcm01 meta     422 Dec  4 15:27 JupyterLab_23.08.o18928489\n",
      "-rw------- 1 sulcm01 meta     422 Dec  6 13:31 JupyterLab_23.08.o19031396\n",
      "-rw------- 1 sulcm01 meta     422 Dec  7 18:13 JupyterLab_23.08.o19046614\n",
      "-rw------- 1 sulcm01 meta     422 Dec  9 16:22 JupyterLab_23.08.o19060535\n",
      "-rw------- 1 sulcm01 meta     422 Dec 10 20:25 JupyterLab_23.08.o19063589\n",
      "-rw------- 1 sulcm01 meta     422 Dec 15 09:05 JupyterLab_23.08.o19114179\n",
      "-rw------- 1 sulcm01 meta     422 Dec 19 09:09 JupyterLab_23.08.o19154839\n",
      "-rw------- 1 sulcm01 meta     422 Dec 20 10:48 JupyterLab_23.08.o19161205\n",
      "-rw------- 1 sulcm01 meta     422 Jan  5 10:28 JupyterLab_23.08.o19430156\n",
      "-rw------- 1 sulcm01 meta     422 Jan  7 12:33 JupyterLab_23.08.o19476417\n",
      "-rw------- 1 sulcm01 meta     422 Jan 12 14:36 JupyterLab_23.08.o19620529\n",
      "-rw------- 1 sulcm01 meta     422 Jan 24 16:18 JupyterLab_23.08.o19821510\n",
      "-rw------- 1 sulcm01 meta     422 Jan 26 11:34 JupyterLab_23.08.o19836725\n",
      "-rw------- 1 sulcm01 meta     422 Jan 28 16:20 JupyterLab_23.08.o19867443\n",
      "-rw------- 1 sulcm01 meta     422 Feb 19 00:07 JupyterLab_23.08.o20223978\n",
      "-rw------- 1 sulcm01 meta     422 Feb 20 19:42 JupyterLab_23.08.o20250938\n",
      "-rw------- 1 sulcm01 meta     422 Feb 22 00:12 JupyterLab_23.08.o20259738\n",
      "-rw-r--r-- 1 sulcm01 meta     183 Oct  4 20:48 README.md\n",
      "-rw-r--r-- 1 sulcm01 meta    3182 Feb 17 15:29 asr_w_spellchecker.py\n",
      "-rw-r--r-- 1 sulcm01 meta    3335 Feb 17 15:29 create_dataset_4_t5.py\n",
      "-rw-r--r-- 1 sulcm01 meta     428 Feb 17 15:29 create_dataset_4_t5.sh\n",
      "-rw-r--r-- 1 sulcm01 meta    5764 Jan  1 11:53 dataset_analysis.ipynb\n",
      "-rw-r--r-- 1 sulcm01 meta    9956 Feb 17 15:29 error_eval.ipynb\n",
      "drwxr-xr-x 2 sulcm01 meta     177 Feb 17 15:29 eval_scripts\n",
      "-rw-r--r-- 1 sulcm01 meta    3424 Feb 17 15:29 model_eval.py\n",
      "-rw-r--r-- 1 sulcm01 meta     121 Feb 17 15:29 requirements.txt\n",
      "drwxr-xr-x 4 sulcm01 meta     134 Feb 17 15:29 run_scripts\n",
      "-rw-r--r-- 1 sulcm01 meta    8533 Feb 17 15:29 test.ipynb\n",
      "-rw-r--r-- 1 sulcm01 meta   49084 Feb 21 16:02 train_nb.ipynb\n"
     ]
    }
   ],
   "source": [
    "! ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/auto/plzen4-ntis/home/sulcm01/Repos/wav2vec2-w-T5-cs/run_scripts\n",
      "total 80\n",
      "-rw-r--r--  1 sulcm01 meta    44 Oct  4 20:48 README.md\n",
      "drwxr-xr-x  4 sulcm01 meta    44 Oct 25 09:43 configs\n",
      "-rw-r--r--  1 sulcm01 meta 31610 Feb 17 15:29 run_seq2seq.py\n",
      "-rw-r--r--  1 sulcm01 meta 32787 Oct 25 09:43 run_speech_recognition_ctc.py\n",
      "drwx------ 48 sulcm01 meta  4096 Feb 21 15:55 wandb\n"
     ]
    }
   ],
   "source": [
    "# move to run script location\n",
    "%cd ./run_scripts\n",
    "! ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wandb login *** API_key ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared to run: configs/t5/finetune_seq2seq_v15.sh\n"
     ]
    }
   ],
   "source": [
    "model_name = \"t5\" # must be wav2vec2 / t5\n",
    "config = \"v15\"\n",
    "\n",
    "if model_name == \"wav2vec2\":\n",
    "    run_training = os.path.join(\"configs\", model_name, f\"finetune_speech_recognition_ctc_{config}.sh\")\n",
    "elif model_name == \"t5\":\n",
    "    run_training = os.path.join(\"configs\", model_name, f\"finetune_seq2seq_{config}.sh\")\n",
    "else:\n",
    "    raise ValueError(f\"Unknown model name\")\n",
    "\n",
    "if not os.path.exists(run_training):\n",
    "    raise ValueError(f\"Unknown config path\")\n",
    "\n",
    "print(f\"Prepared to run: {run_training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-22 09:20:53.357578: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "02/22/2024 09:21:49 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/22/2024 09:21:49 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=8,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=1000,\n",
      "evaluation_strategy=steps,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=64,\n",
      "generation_num_beams=4,\n",
      "gradient_accumulation_steps=8,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.01,\n",
      "learning_rate=0.0015,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/storage/plzen4-ntis/home/sulcm01/outputs/t5/t5-spellchecker-cs-v15/runs/Feb22_09-21-49_galdor18.metacentrum.cz,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=100,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=20000,\n",
      "metric_for_best_model=wer,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=/storage/plzen4-ntis/home/sulcm01/outputs/t5/t5-spellchecker-cs-v15/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=16,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard', 'wandb'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/storage/plzen4-ntis/home/sulcm01/outputs/t5/t5-spellchecker-cs-v15/,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=1000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=800,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "[INFO|configuration_utils.py:713] 2024-02-22 09:21:50,375 >> loading configuration file /storage/plzen4-ntis/projects/public/Lehecka/t5_32k_cccs_jmzw.v2/config.json\n",
      "[INFO|configuration_utils.py:775] 2024-02-22 09:21:50,558 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"/storage/plzen4-ntis/projects/public/Lehecka/t5_32k_cccs_jmzw.v2\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.34.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2013] 2024-02-22 09:21:50,606 >> loading file spiece.model\n",
      "[INFO|tokenization_utils_base.py:2013] 2024-02-22 09:21:50,606 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2013] 2024-02-22 09:21:50,606 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2013] 2024-02-22 09:21:50,606 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2013] 2024-02-22 09:21:50,606 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:305] 2024-02-22 09:21:50,699 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "[INFO|modeling_utils.py:2990] 2024-02-22 09:21:52,423 >> loading weights file /storage/plzen4-ntis/projects/public/Lehecka/t5_32k_cccs_jmzw.v2/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:770] 2024-02-22 09:22:14,148 >> Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3775] 2024-02-22 09:22:15,551 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:3783] 2024-02-22 09:22:15,551 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at /storage/plzen4-ntis/projects/public/Lehecka/t5_32k_cccs_jmzw.v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "[INFO|modeling_utils.py:3352] 2024-02-22 09:22:15,738 >> Generation config file not found, using a generation config created from the model config.\n",
      "Process #0 will write at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/train/cache-90357429fa9727f3_00000_of_00008.arrow\n",
      "02/22/2024 09:22:16 - INFO - datasets.arrow_dataset - Process #0 will write at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/train/cache-90357429fa9727f3_00000_of_00008.arrow\n",
      "Process #1 will write at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/train/cache-90357429fa9727f3_00001_of_00008.arrow\n",
      "02/22/2024 09:22:16 - INFO - datasets.arrow_dataset - Process #1 will write at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/train/cache-90357429fa9727f3_00001_of_00008.arrow\n",
      "Process #2 will write at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/train/cache-90357429fa9727f3_00002_of_00008.arrow\n",
      "02/22/2024 09:22:16 - INFO - datasets.arrow_dataset - Process #2 will write at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/train/cache-90357429fa9727f3_00002_of_00008.arrow\n",
      "Process #3 will write at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/train/cache-90357429fa9727f3_00003_of_00008.arrow\n",
      "02/22/2024 09:22:16 - INFO - datasets.arrow_dataset - Process #3 will write at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/train/cache-90357429fa9727f3_00003_of_00008.arrow\n",
      "Process #4 will write at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/train/cache-90357429fa9727f3_00004_of_00008.arrow\n",
      "02/22/2024 09:22:16 - INFO - datasets.arrow_dataset - Process #4 will write at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/train/cache-90357429fa9727f3_00004_of_00008.arrow\n",
      "Process #5 will write at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/train/cache-90357429fa9727f3_00005_of_00008.arrow\n",
      "02/22/2024 09:22:16 - INFO - datasets.arrow_dataset - Process #5 will write at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/train/cache-90357429fa9727f3_00005_of_00008.arrow\n",
      "Process #6 will write at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/train/cache-90357429fa9727f3_00006_of_00008.arrow\n",
      "02/22/2024 09:22:16 - INFO - datasets.arrow_dataset - Process #6 will write at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/train/cache-90357429fa9727f3_00006_of_00008.arrow\n",
      "Process #7 will write at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/train/cache-90357429fa9727f3_00007_of_00008.arrow\n",
      "02/22/2024 09:22:16 - INFO - datasets.arrow_dataset - Process #7 will write at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/train/cache-90357429fa9727f3_00007_of_00008.arrow\n",
      "Loading cached processed dataset at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/train/cache-90357429fa9727f3_*_of_00008.arrow\n",
      "02/22/2024 09:22:19 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/train/cache-90357429fa9727f3_*_of_00008.arrow\n",
      "Concatenating 8 shards\n",
      "02/22/2024 09:22:19 - INFO - datasets.arrow_dataset - Concatenating 8 shards\n",
      "Process #0 will write at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/validation/cache-b82dec8883ec5840_00000_of_00008.arrow\n",
      "02/22/2024 09:22:19 - INFO - datasets.arrow_dataset - Process #0 will write at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/validation/cache-b82dec8883ec5840_00000_of_00008.arrow\n",
      "Process #1 will write at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/validation/cache-b82dec8883ec5840_00001_of_00008.arrow\n",
      "02/22/2024 09:22:19 - INFO - datasets.arrow_dataset - Process #1 will write at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/validation/cache-b82dec8883ec5840_00001_of_00008.arrow\n",
      "Process #2 will write at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/validation/cache-b82dec8883ec5840_00002_of_00008.arrow\n",
      "02/22/2024 09:22:19 - INFO - datasets.arrow_dataset - Process #2 will write at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/validation/cache-b82dec8883ec5840_00002_of_00008.arrow\n",
      "Process #3 will write at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/validation/cache-b82dec8883ec5840_00003_of_00008.arrow\n",
      "02/22/2024 09:22:19 - INFO - datasets.arrow_dataset - Process #3 will write at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/validation/cache-b82dec8883ec5840_00003_of_00008.arrow\n",
      "Process #4 will write at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/validation/cache-b82dec8883ec5840_00004_of_00008.arrow\n",
      "02/22/2024 09:22:19 - INFO - datasets.arrow_dataset - Process #4 will write at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/validation/cache-b82dec8883ec5840_00004_of_00008.arrow\n",
      "Process #5 will write at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/validation/cache-b82dec8883ec5840_00005_of_00008.arrow\n",
      "02/22/2024 09:22:19 - INFO - datasets.arrow_dataset - Process #5 will write at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/validation/cache-b82dec8883ec5840_00005_of_00008.arrow\n",
      "Process #6 will write at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/validation/cache-b82dec8883ec5840_00006_of_00008.arrow\n",
      "02/22/2024 09:22:19 - INFO - datasets.arrow_dataset - Process #6 will write at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/validation/cache-b82dec8883ec5840_00006_of_00008.arrow\n",
      "Process #7 will write at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/validation/cache-b82dec8883ec5840_00007_of_00008.arrow\n",
      "02/22/2024 09:22:19 - INFO - datasets.arrow_dataset - Process #7 will write at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/validation/cache-b82dec8883ec5840_00007_of_00008.arrow\n",
      "Loading cached processed dataset at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/validation/cache-b82dec8883ec5840_*_of_00008.arrow\n",
      "02/22/2024 09:22:21 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /storage/plzen4-ntis/home/sulcm01/datasets/t5/asr-correction-cs-v23/validation/cache-b82dec8883ec5840_*_of_00008.arrow\n",
      "Concatenating 8 shards\n",
      "02/22/2024 09:22:21 - INFO - datasets.arrow_dataset - Concatenating 8 shards\n",
      "[INFO|trainer.py:576] 2024-02-22 09:22:42,332 >> max_steps is given, it will override any value given in num_train_epochs\n",
      "[INFO|trainer.py:1760] 2024-02-22 09:22:42,508 >> ***** Running training *****\n",
      "[INFO|trainer.py:1761] 2024-02-22 09:22:42,508 >>   Num examples = 18,902\n",
      "[INFO|trainer.py:1762] 2024-02-22 09:22:42,508 >>   Num Epochs = 137\n",
      "[INFO|trainer.py:1763] 2024-02-22 09:22:42,508 >>   Instantaneous batch size per device = 16\n",
      "[INFO|trainer.py:1766] 2024-02-22 09:22:42,508 >>   Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "[INFO|trainer.py:1767] 2024-02-22 09:22:42,508 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:1768] 2024-02-22 09:22:42,508 >>   Total optimization steps = 20,000\n",
      "[INFO|trainer.py:1769] 2024-02-22 09:22:42,509 >>   Number of trainable parameters = 222,903,552\n",
      "[INFO|integration_utils.py:722] 2024-02-22 09:22:42,547 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msulcm\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.3 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/auto/plzen4-ntis/home/sulcm01/Repos/wav2vec2-w-T5-cs/run_scripts/wandb/run-20240222_092243-rqxbepf2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msparkling-dumpling-76\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/sulcm/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/sulcm/huggingface/runs/rqxbepf2\u001b[0m\n",
      "  0%|                                                 | 0/20000 [00:00<?, ?it/s][WARNING|logging.py:290] 2024-02-22 09:22:45,719 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:290] 2024-02-22 09:22:45,848 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:290] 2024-02-22 09:22:45,883 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:290] 2024-02-22 09:22:45,888 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:290] 2024-02-22 09:22:45,904 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:290] 2024-02-22 09:22:45,910 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:290] 2024-02-22 09:22:45,910 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:290] 2024-02-22 09:22:45,913 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  0%|                                      | 15/20000 [00:18<5:51:32,  1.06s/it]"
     ]
    }
   ],
   "source": [
    "! bash {run_training}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
